# description: default app level settings


## App defaults
#
[calamr_default]
# data cache directory
data_dir = ${default:data_dir}/calamr
# the directory for local results (paper)
results_dir = ${default:root_dir}/results
# default wordpiece feature document parser
word_piece_doc_factory = word_piece_doc_factory
# the stash that provides annotated (for source/summary) AMR graphs
amr_anon_doc_stash = calamr_anon_feature_doc_stash
# the directory of AMR release corpora
amr_rel_dir = ${amr_anon_default:corpus_dir}/amr-rel
# whether to just report or fix reentrancies by redirecting flow
only_report_reentrancies = False
# number of workers to parse documents
amr_parse_preemptive_workers = 1
# number of workers to align graphs (when flow_graph_result_caching = preemptive)
align_preemptive_workers = 1
# the type of caching: either 'lazy' or 'preemptive'
flow_graph_result_caching = preemptive
# alignment graph rendering: flat ('graphviz)' vs 3D ('plotly' or 'pyvis')
renderer = graphviz


## Alignment hyperparameters
#
[calamr_alignment]
# the minimum amount of flow to trigger setting the capacity of a target edge
# capacity to `capacity_min`
align_min_capacity_cutoff = 0.1
# finds low flow role edges at or lower than this value and sets (zeros out)
# all the capacities of all the connected edge aligmnets recursively for all
# descendants
role_min_flow_cutoff = 0.1
# the graph component alignment edges are removed if their capacities are at
# or below this value, which is set by anything lower than
# `min_capacity_cutoff` after the graph sequence completes on converge
capacity_min = 0


## Alignment graph rendering
#
# the extension to write files by graphviz, which can be eps, svg, pdf, etc.
[calamr_graph_render_graphviz]
extension = svg


## Adhoc corpus
#
# the name of the default adhoc corpus, which by default has the input file:
# `corpus/<name>/source.json`; see resources/corpus/adhoc.conf
[calamr_adhoc_corpus_default]
name = micro


## AMR
#
# set the default parse model (set in upstream config instead)
#[amr_default]
#parse_model = spring


## Deepnlp
#
# globally cache a single instance of the sentence embeddings
#[transformer_sent_fixed_resource]
#cache = True
